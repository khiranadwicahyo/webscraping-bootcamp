{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7acce5bd",
   "metadata": {},
   "source": [
    "## **Basics Python for Scraping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1daa938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58fb7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['Indonesia', 'Malaysia', 'Philippines', 'Thailand', 'Vietnam']\n",
    "id_states = ['INA', 'MY', 'PH', 'TH', 'VN']\n",
    "\n",
    "dict_states = {'States': states, 'ID States': id_states}\n",
    "\n",
    "df_states = pd.DataFrame.from_dict(dict_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f451d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>States</th>\n",
       "      <th>ID States</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indonesia</td>\n",
       "      <td>INA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Malaysia</td>\n",
       "      <td>MY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Philippines</td>\n",
       "      <td>PH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>TH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vietnam</td>\n",
       "      <td>VN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        States ID States\n",
       "0    Indonesia       INA\n",
       "1     Malaysia        MY\n",
       "2  Philippines        PH\n",
       "3     Thailand        TH\n",
       "4      Vietnam        VN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7580e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can etiher export to csv or excel\n",
    "# df_states.to_csv('states.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f17464",
   "metadata": {},
   "source": [
    "### **Handling Exception Errors Using try-except**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa808a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "the element is not a number!\n"
     ]
    }
   ],
   "source": [
    "a_list = [2,4,6, 'string element']\n",
    "\n",
    "for element in a_list:\n",
    "    try:\n",
    "        print(element/2)\n",
    "    except:\n",
    "        print('the element is not a number!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af5fb14",
   "metadata": {},
   "source": [
    "Ketika kita melakukan scraping terkadang kita menemukan error pada elemen yang ingin kita ambil karena ketidak sesuaian data, sebagai contoh pada kasus try-except digunakan agar code yang kita bisa berjalan hingga akhir walaupun elemen yang ingin kita ambil tidak memenuhi kriteria.\n",
    "\n",
    "Jika try-excepet tidak digunakan code akan mengeluarkan error messages sehingga data yang berhasil diambil tidak dapat tersimpan karena salah satu elemen tidak sesuai kriteria, maka dari itu try-except digunakan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205e352e",
   "metadata": {},
   "source": [
    "### **Pagination Using while-break**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95351453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "End of the pages loop\n"
     ]
    }
   ],
   "source": [
    "num_pages = 5 # number of pages\n",
    "\n",
    "while num_pages > 0 : # expception\n",
    "    print(num_pages)\n",
    "    num_pages -= 1\n",
    "\n",
    "    if num_pages == 2:\n",
    "        break\n",
    "print('End of the pages loop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ffb94d",
   "metadata": {},
   "source": [
    "**Pagination** merupakan cara untuk kita mengambil data dari setiap halaman website yang ada. Umumnya digunakan pada librarry Beautiful soup dan Selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c5d69",
   "metadata": {},
   "source": [
    "## **Basics Scraping With Beautifulsoup4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e74c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4185786",
   "metadata": {},
   "source": [
    "### **What we need to know about Beautifulsoup4 (Bs4)?**\n",
    "\n",
    "Bs4 merupakan librarry yang digunakan untuk melakukan web scraping pada suatu website. Dalam penggunaan Bs4 kita bergantung pada *Request* dari suatu website untuk mendapatkan data yang kita mau. Meskipun terlihat simpel Bs4 merupakan web scraper yang juga membutuhkan *parser* seperti **html parser** atau **xml parser** untuk mengekstrak data dari suatu website. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aa97ac",
   "metadata": {},
   "source": [
    "### **Scraping Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd0ed0",
   "metadata": {},
   "source": [
    "Biasanya dalam scraping menggunakan Bs4 kita membutuhkan:\n",
    "\n",
    "    1. Inisiasi URL\n",
    "    2. Buat Request\n",
    "    3. Parsing Data\n",
    "    4. Ekstraksi Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b62fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat insisiaisi website\n",
    "website = 'https://subslikescript.com/movie/The_Love_Club-19896920'\n",
    "# melakukan request ke website\n",
    "result = requests.get(website)\n",
    "# menggambil konten dari hasil request\n",
    "content = result.text\n",
    "\n",
    "# membuat soup untuk parsing lxml/html\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "# memuncuklan hasil parsing\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c09ce",
   "metadata": {},
   "source": [
    "**Screenshot website yang ingin dilakukan scraping**\n",
    "\n",
    "![media - website subslikescript](../media/basics_scraping_subslikescript.png)\n",
    "\n",
    "Untuk melihat elemen â€” kita bisa melakukan inspect langsung pada website dan pilih elemen yang ingin diambil datanya. Pada contoh kali ini saya ingin mengambil data judul dari film \"THE LOVE CLUB (2023) - FULL TRANSCRIPT\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4acfe822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Love Club (2023) - full transcript\n"
     ]
    }
   ],
   "source": [
    "# karena judul film berada di dalam tag <h1> dan <article>\n",
    "# kita perlu mencari tag sebelumnya yaitu <article>\n",
    "box = soup.find('article', class_='main-article')\n",
    "# mengambil judul film dari tag <h1>\n",
    "title = box.find('h1').text\n",
    "# print hasil judul film\n",
    "print(f'Title: {title}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selanjutnya kita akan mengambil transkrip film dan membuat file txtnya\n",
    "\n",
    "# mengambil transkrip film dari tag <div> dengan class 'full-script'\n",
    "transcript = box.find('div', class_='full-script').get_text(strip=True, separator=' ')\n",
    "\n",
    "# membuat file\n",
    "# ketika membuat file, gunakan mode 'w' untuk menulis\n",
    "# dan encoding 'utf-8' untuk mendukung karakter non-ASCII\n",
    "# pastikan direktori '../data export/' sudah ada\n",
    "# jika belum ada, buat direktori tersebut terlebih dahulu\n",
    "# dalah contoh ini, kita akan menyimpan file dengan nama sesuai judul film\n",
    "with open(f'../data export/{title}.txt', 'w', encoding= 'utf-8') as file:\n",
    "    file.write(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0390f6",
   "metadata": {},
   "source": [
    "### **Scraping Multiple Element**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e60dab8",
   "metadata": {},
   "source": [
    "![subslikescript - multiple element href true](../media/href_true.png)\n",
    "\n",
    "\n",
    "![subslikescript - href zoom](../media/href_zoom.png)\n",
    "\n",
    "Dalam contoh kali ini, untuk mengambil beberapa element yang sama kita perlu memahami struktur html dari suatu website. Jika diperhatikan **href** menunjukkan bahwa setiap movies akan menampilkan tampilan transcript dihalaman yang terpisah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77735c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/movie/Gereza-22857380', '/movie/Dragon_Hunter-22008250', '/movie/House_of_Inequity-5598934', '/movie/Jackpot_Island_Kumanthong_Returns-25260658', '/movie/Match-13490000', '/movie/Snow_White_and_the_Fairytale_Fun_Force-22700808', '/movie/The_Love_Club-19896920', '/movie/Haunted_Hotties-24018636', '/movie/One_Year_Off-14130940', '/movie/Sweeter_Than_Chocolate-25391002', '/movie/Disquiet-25869142', '/movie/Among_the_Beasts-26343318', '/movie/Taming_Speed-25969676', '/movie/Randy_Feltface_The_Last_Temptation_of_Randy-17527566', '/movie/Alemanji-23666980', '/movie/Darkheart_Manor-12569866', '/movie/Innocent_Vengeance-26505553', '/movie/Ho_Ja_Mukt-25150506', '/movie/Boy_from_Nowhere-26713915', '/movie/Rent-a-Groom-15352290', '/movie/Unlocked-26160190', '/movie/j-hope_IN_THE_BOX-26425683', '/movie/Suki-26734145', '/movie/Re-Resonator_Looking_Back_at_from_Beyond-26625455', '/movie/Americas_National_Parks_at_100-26740893', '/movie/Under_His_Influence-26083016', '/movie/Why_Cant_My_Life_Be_a_Rom_Com-20912894', '/movie/Nasty-22500124', '/movie/Kathleen_Madigan_Hunting_Bigfoot-26348233', '/movie/Cassius_X_Becoming_Ali-17001434']\n"
     ]
    }
   ],
   "source": [
    "# membuat insisiaisi website\n",
    "website = 'https://subslikescript.com/movies'\n",
    "# melakukan request ke website\n",
    "result = requests.get(website)\n",
    "# menggambil konten dari hasil request\n",
    "content = result.text\n",
    "\n",
    "# membuat soup untuk parsing lxml/html\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "box = soup.find('article', class_='main-article')\n",
    "\n",
    "links = []\n",
    "\n",
    "for link in box.find_all('a', href=True):\n",
    "    # mengambil link dari atribut href\n",
    "    links.append(link['href'])\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30648183",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: \"../data export/Why Can't My Life Be a Rom Com? (2023) - full transcript.txt\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# mengambil transkrip film dari tag <div> dengan class 'full-script'\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     transcript \u001b[38;5;241m=\u001b[39m box\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull-script\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data export/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtitle\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     19\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(transcript)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# menampilkan pesan selesai\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: \"../data export/Why Can't My Life Be a Rom Com? (2023) - full transcript.txt\""
     ]
    }
   ],
   "source": [
    "root = 'https://subslikescript.com'\n",
    "# mengambil setiap link dan mengambil transkrip filmnya menggunakan loop\n",
    "for link in links:\n",
    "    movies = f'{root}{link}'\n",
    "    # melakukan request ke website\n",
    "    result = requests.get(movies)\n",
    "    # menggambil konten dari hasil request\n",
    "    content = result.text\n",
    "    # membuat soup untuk parsing lxml/html\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "    box = soup.find('article', class_='main-article')\n",
    "    # mengambil judul film dari tag <h1>\n",
    "    title = box.find('h1').text\n",
    "    # mengambil transkrip film dari tag <div> dengan class 'full-script'\n",
    "    transcript = box.find('div', class_='full-script').get_text(strip=True, separator=' ')\n",
    "\n",
    "    with open(f'../data export/{title}.txt', 'w', encoding= 'utf-8') as file:\n",
    "        file.write(transcript)\n",
    "        \n",
    "# menampilkan pesan selesai\n",
    "print('Semua transkrip film telah diambil dan disimpan!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b1b557",
   "metadata": {},
   "source": [
    "Jika kita lihat diatas terdapat error yang disebabkan karena adanya karakter ilegal untuk membuat nama file. Kita bisa menggunakan try-except untuk solusi seperti ini atau gunakaan fungsi regex untuk membersihkan karakter yang tidak aman, tetapi fokus utama dalam mengambil multiple element sudah berhasil diterapkan. jika kita cek di directory file sudah berhasil dibuat dan diambil datanya.\n",
    "\n",
    "![multiple element scraping](../media/multiple_element_scraping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3800c743",
   "metadata": {},
   "source": [
    "### **Pagination Using Bs4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15c1d2d",
   "metadata": {},
   "source": [
    "![pagination element](../media/pagination.png)\n",
    "\n",
    "Untuk melakukan pagination kita perlu mangambil elemen yang memuat akses ke halama berikutnya. Dalam website \"subslikescript.com\" dapan dilihat elemen berapa pada box ul dan link pada li. Jika diperhatikan juga situs akan berubah menjadi \"subslikescript.com/**movies?page=2**\" artinya setiap perubahan ke halaman berikutnya situs akan berubah menjadi seperti ini. Jika sudah diketahui karakteristik dari web ketika berpindah ke halaman berikutnya kita bisa mengakali dengan mengambil banyak halaman dan memasukkannya dengan angka pada **page={}**.\n",
    "\n",
    "Sayangnya website ini telah membatasi aktivitas seperti ini, karena sistem website akan mengenali pergerakan tidak wajar dari user. Walaupun mungkin bisa diatasi menggunakan fungsi time untuk melakukan jeda pada proses scraping, akan tetapi tidak ada jaminan data dapat terambil semua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e370c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff7bf4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "492ee27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transkrip untuk Gereza (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Dragon Hunter (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk House of Inequity (2021) - full transcript telah disimpan.\n",
      "Transkrip untuk Jackpot Island Kumanthong Returns (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Match (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Snow White and the Fairytale Fun Force (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk The Love Club (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Haunted Hotties (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk One Year Off (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Sweeter Than Chocolate (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Disquiet (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Among the Beasts (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Taming Speed (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Randy Feltface The Last Temptation of Randy (2020) - full transcript telah disimpan.\n",
      "Transkrip untuk Alemanji (2021) - full transcript telah disimpan.\n",
      "Transkrip untuk Darkheart Manor (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Innocent Vengeance (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Ho Ja Mukt (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Boy from Nowhere (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Rent-a-Groom (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Unlocked (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk j-hope IN THE BOX (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Suki (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Re-Resonator Looking Back at from Beyond (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk America's National Parks at 100 (2016) - full transcript telah disimpan.\n",
      "Transkrip untuk Under His Influence (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Why Can't My Life Be a Rom Com (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Nasty (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Kathleen Madigan Hunting Bigfoot (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Cassius X Becoming Ali (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Verden er uskarp (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Bullet to Beijing (1995) - full transcript telah disimpan.\n",
      "Transkrip untuk Whindersson Nunes Isso NÃ£o Ã© um Culto (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Fursat (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk The Weeknd Live at SoFi Stadium (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk The Adventures of Jurassic Pet The Lost Secret (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Casando a mi Ex (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Tonight You're Sleeping with Me (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Batman The Doom That Came to Gotham (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk South Park Post COVID - The Return of COVID (2021) - full transcript telah disimpan.\n",
      "Transkrip untuk Fall (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Black Girl Missing (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Royal Rendezvous (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Love Sick Open All Day, Every Night (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Marc Maron From Bleak to Dark (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Pink Moon (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Lady Behave! (1937) - full transcript telah disimpan.\n",
      "Transkrip untuk The Legend & Butterfly (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Kalevala - uusi aika (2013) - full transcript telah disimpan.\n",
      "Transkrip untuk Face Off The Walking Guests () - full transcript telah disimpan.\n",
      "Transkrip untuk Revenge of the First Wives (1997) - full transcript telah disimpan.\n",
      "Transkrip untuk The London Nobody Knows (1968) - full transcript telah disimpan.\n",
      "Transkrip untuk Building Ireland (2017) - full transcript telah disimpan.\n",
      "Transkrip untuk The Girl on a Motorcycle (1968) - full transcript telah disimpan.\n",
      "Transkrip untuk Taz Quest for Burger (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Trust Us (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Swooner Crooner (1944) - full transcript telah disimpan.\n",
      "Transkrip untuk Plane Daffy (1944) - full transcript telah disimpan.\n",
      "Transkrip untuk A Gruesome Twosome (1945) - full transcript telah disimpan.\n",
      "Transkrip untuk I Am T-Rex (2022) - full transcript telah disimpan.\n"
     ]
    }
   ],
   "source": [
    "root = 'https://subslikescript.com'\n",
    "base_url = 'https://subslikescript.com/movies'\n",
    "result = requests.get(base_url)\n",
    "# menggambil konten dari hasil request\n",
    "content = result.text\n",
    "\n",
    "# membuat soup untuk parsing lxml/html\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "# pagination adalah elemen yang berisi link ke halaman-halaman berikutnya\n",
    "pagination = soup.find('ul', class_='pagination')\n",
    "pages = pagination.find_all('li', class_='page-item')\n",
    "last_pages = pages[-2].text  # mengambil jumlah halaman terakhir\n",
    "\n",
    "\n",
    "# for page in range(1, int(last_pages) + 1)[:2]: # [:2] mengambil 2 halaman pertama\n",
    "for page in range(1, 3): \n",
    "    try:\n",
    "        get_ref = requests.get(f'{base_url}?page={page}')\n",
    "        pg_content = get_ref.text\n",
    "        soup = BeautifulSoup(pg_content, 'lxml')\n",
    "\n",
    "        box = soup.find('article', class_='main-article')\n",
    "        # mengambil link dari setiap halaman\n",
    "        # for link in box.find_all('a', href=True):\n",
    "        # # mengambil link dari atribut href\n",
    "        #     links.append(link['href'])\n",
    "\n",
    "        # reset link untuk tiap halaman\n",
    "        links = [link['href'] for link in box.find_all('a', href=True)]\n",
    "    \n",
    "        # mengambil setiap link dan mengambil transkrip filmnya menggunakan loop\n",
    "        for link in links:\n",
    "            try:\n",
    "                # melakukan request ke website menggunakan link yang telah diambil\n",
    "                result = requests.get(f'{root}{link}')\n",
    "                time.sleep(2)  # menunggu 2 detik untuk menghindari terlalu banyak request\n",
    "                # menggambil konten dari hasil request\n",
    "                content = result.text\n",
    "                # membuat soup untuk parsing lxml/html\n",
    "                soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "                box = soup.find('article', class_='main-article')\n",
    "                # mengambil judul film dari tag <h1>\n",
    "                title = box.find('h1').text\n",
    "                # membersihkan judul film dari karakter yang tidak diinginkan\n",
    "                safe_title = clean_text(title)\n",
    "                # mengambil transkrip film dari tag <div> dengan class 'full-script'\n",
    "                transcript = box.find('div', class_='full-script').get_text(strip=True, separator=' ')\n",
    "\n",
    "                with open(f'../data export/{safe_title}.txt', 'w', encoding= 'utf-8') as file:\n",
    "                    file.write(transcript)\n",
    "                print(f'Transkrip untuk {safe_title} telah disimpan.')\n",
    "            except Exception as e:\n",
    "                print(f'Error pada link {link}: {e}')\n",
    "                continue\n",
    "    except:\n",
    "        print(f'Error pada halaman {page}, melanjutkan ke halaman berikutnya.')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f4c6e9",
   "metadata": {},
   "source": [
    "## **Xpath Introduction to Scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e12e1",
   "metadata": {},
   "source": [
    "### **Xpath Syntax**\n",
    "\n",
    "Example of Xpath Syntax you need to know: \n",
    "\n",
    "**//TagName[contains(@AtributName, 'Value')]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3787318",
   "metadata": {},
   "source": [
    "### **Xpath Operation**\n",
    "\n",
    "Example of Xpath Operator\n",
    "\n",
    "**//TagName[(Expression 1) and (Expression 2)]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf586e",
   "metadata": {},
   "source": [
    "### **HTML Code Example** \n",
    "\n",
    "```HTML\n",
    "<body>\n",
    "\t<article class=\"main-article\" style=\"height: auto !important;\">\n",
    "\t\t\t<h1>Vacation Home Nightmare (2023) - full transcript</h1>\n",
    "\t\t\t\t\t\t<p class=\"plot\">When a woman is attacked in her short term rental, the company's Clean-Up Team steps in to help her pick up the pieces. But she soon finds that they might not be who they say they are.</p>\n",
    "\t</article>\n",
    "\n",
    "\t<div id=\"cue-app\" class=\"full-script\" data-imdb-id=\"26458228\">\n",
    "\t\t\t\t<div class=\"subtitle-cue\">\n",
    "\t\t\t\t\t\t<p class=\"cue-line\" data-cue-idx=\"0\" data-line-idx=\"0\">(dark music)</p>\n",
    "\t\t\t\t\t</div>\n",
    "\t\t\t\t<div class=\"subtitle-cue\">\n",
    "\t\t\t\t\t\t<p class=\"cue-line\" data-cue-idx=\"1\" data-line-idx=\"0\">â™ª</p>\n",
    "\t\t\t\t\t</div>\n",
    "\t\t\t\t<div class=\"subtitle-cue\">\n",
    "\t\t\t\t\t\t<p class=\"cue-line\" data-cue-idx=\"2\" data-line-idx=\"0\">Yeah, it's so quiet out here,</p>\n",
    "\t\t\t\t\t\t<p class=\"cue-line\" data-cue-idx=\"2\" data-line-idx=\"1\">I feel like I'm all alone.</p>\n",
    "\t\t\t\t\t</div>\n",
    "\t</div>\n",
    "\t<footer>\n",
    "\t\t\t<span class=\"contactus\">Have any questions? Contact us: subslikescript(doggysign)gmail.com | <a href=\"https://subslikescript.com/dmca\">DMCA</a></span>\n",
    "\t</footer>\n",
    "</body>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771bec4",
   "metadata": {},
   "source": [
    "**contoh penggunaan sytanx Xpath untuk kode diatas:**\n",
    "\n",
    "- Untuk mengambil keseluruhan isi tag article\n",
    "\n",
    "    kita dapat menggunakan syntax:\n",
    "\n",
    "    > //article\n",
    "     \n",
    "    >//article[@class=\"main-article\"]\n",
    "    \n",
    "\n",
    "- Untuk mengamil tag h1\n",
    "    \n",
    "    kita dapat menggunakan syntax:\n",
    "\n",
    "    > //article[@class=\"main-article\"]/h1/text()\n",
    "\n",
    "    hasil akan seperti ini : Vacation Home Nightmare (2023) - full transcript\n",
    "\n",
    "- Untuk mengambil text script yang ada di p \n",
    "     \n",
    "    kita dapat menggunakan syntax:\n",
    "\n",
    "    > //p[contains(@data-cue-idx , \"0\")]\n",
    "\n",
    "    atau \n",
    "\n",
    "    > //p[contains(@class , \"cue\")]\n",
    "\n",
    "    hasil akan seperti ini : \n",
    "    \n",
    "    ```html\n",
    "    <p class=\"cue-line\" data-cue-idx=\"0\" data-line-idx=\"0\">(dark music)</p>\n",
    "    <p class=\"cue-line\" data-cue-idx=\"1\" data-line-idx=\"0\">â™ª</p>\n",
    "    <p class=\"cue-line\" data-cue-idx=\"2\" data-line-idx=\"0\">Yeah, it's so quiet out here,</p>\n",
    "    <p class=\"cue-line\" data-cue-idx=\"2\" data-line-idx=\"1\">I feel like I'm all alone.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d336b",
   "metadata": {},
   "source": [
    "### **Special Characters Xpath Syntax**\n",
    "\n",
    "* **/. atau /..** -> digunakan untuk mengambil node parents \n",
    "* **/***-> digunakan untuk mengambil seluruh elemen tidap penting apapun elemennya\n",
    "* **./*** -> digunakan untuk mengambil seluruh elemen dari childern node\n",
    "* **@** -> untuk memilih attribut\n",
    "* **()** -> untuk grouping xpath expression\n",
    "* **[n]** -> untuk mengindikasikan setiap node dengan index 'n' akan diambil  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b8ebb8",
   "metadata": {},
   "source": [
    "## **Introduction to Selenium Scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf383b5",
   "metadata": {},
   "source": [
    "Selenium merupakan library seperti Bs4 yang juga berfungsi untuk melakukan scraping. Bedanya Selenium melakukan pendekatakn scraping menggunakan Webdriver. Bs4 dan Selenium memiliki target jenis website yang berbeda. Scraping menggunakan Bs4 tidak bisa dilakukan pada website yang dinamis (contoh menggunakan JavaScipt), berbeda dengan Selenium yang mampu melakukan scraping data pada website dinamis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106c4fb",
   "metadata": {},
   "source": [
    "### **Set Up Package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a76238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac13fe47",
   "metadata": {},
   "source": [
    "### **Running Webdriver to Get Website**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1baa3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi path ke file chromedriver.exe\n",
    "path = \"../chromedriver-win64/chromedriver.exe\"\n",
    "\n",
    "# Set service\n",
    "service = Service(executable_path=path)\n",
    "\n",
    "# Opsi tambahan (opsional, tapi berguna)\n",
    "options = Options()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "# Inisialisasi driver\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Buka website\n",
    "# driver.get(\"https://www.vlr.gg/matches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd82afc3",
   "metadata": {},
   "source": [
    "### **How to Find Element with Selenium**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b297ad09",
   "metadata": {},
   "source": [
    "Biasanya untuk mengambil elemen kita menggunakan kode seperti:\n",
    "\n",
    "```Python\n",
    "driver.find_element_by_id(\"id\")\n",
    "driver.find_element_by_class_name(\"class_name\")\n",
    "driver.find_element_by_tag_name(\"tag_name\")\n",
    "driver.find_element_by_xpath(\"//@xpath syntax\")\n",
    "driver.find_elements_by_tag_name(\"tag_name\") # for multiple write \"elements\" for plural "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832213e2",
   "metadata": {},
   "source": [
    "Target website untuk kasus ini menggunakan website dari https://vlr.gg/matches\n",
    "\n",
    "![valorant matches](../media/vlrgg_website.png)\n",
    "\n",
    "kali ini kita akan mencoba menggunakan selenium untuk mengambil dan melakukan action pada driver untuk mengambil button **Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d24da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi driver\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Buka driver\n",
    "driver.get(\"https://www.vlr.gg/matches\")\n",
    "\n",
    "# buat driver untuk mengklik elemen yang ada di dalam website\n",
    "result_mathces = driver.find_element(By.XPATH, '//div[@class=\"wf-nav\"]/a[@href=\"/matches/results\"]/div[@class=\"wf-nav-item-title\"]')\n",
    "# klik elemen yang telah diambil\n",
    "result_mathces.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70146427",
   "metadata": {},
   "source": [
    "### **Scraping Table with Selenium and Export Data to CSV**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e411e09a",
   "metadata": {},
   "source": [
    "Website ini berisikan statistik data player Valorant Region Pacific. Didalam website tersebut bisa kita lihat data disimpan dalam bentuk tabel.\n",
    "\n",
    "![valorant-stats-table](../media/vlrgg_stats_table.png)\n",
    "\n",
    "Data-data tersebut tersimpan dalam tag html \n",
    "``` html\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <td></td>\n",
    "        </tr>\n",
    "    </thead>\n",
    "</table>\n",
    "```    \n",
    "![valorant-stats-table-zoom](../media/vlrgg_stats_table_zoom.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16cede06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data statistik Valorant telah disimpan ke val_stats.csv!\n"
     ]
    }
   ],
   "source": [
    "# Inisialisasi driver\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Buka driver\n",
    "driver.get(\"https://www.vlr.gg/event/stats/2379/champions-tour-2025-pacific-stage-1\")\n",
    "\n",
    "\n",
    "time.sleep(7)  # Tunggu beberapa detik agar halaman sepenuhnya dimuat\n",
    "\n",
    "rows = driver.find_elements(By.XPATH, '//table/tbody/tr')\n",
    "\n",
    "# tulis data ke dalam file csv\n",
    "with open('../data export/val_stats.csv', 'w', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Tulis header kolom sesuai dengan struktur tabel\n",
    "    writer.writerow([\"Player\", \"Agents\", \"RND\", \"R\", \"ACS\", \n",
    "                     \"K:D\", \"KAST\", \"ADR\", \"KPR\", \"APR\", \"FKPR\", \"FDPR\", \"HS%\", \n",
    "                     \"CL%\", \"CL\", \"KMAX\", \"K\", \"D\", \"A\", \"FK\", \"FD\"])\n",
    "    \n",
    "    for row in rows:\n",
    "        cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        # Ekstrak teks dari setiap sel\n",
    "        data = [cell.text.strip() for cell in cells]\n",
    "        writer.writerow(data)\n",
    "\n",
    "driver.quit()  # Tutup driver setelah selesai\n",
    "# Menampilkan pesan selesai\n",
    "print('Data statistik Valorant telah disimpan ke val_stats.csv!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7effe79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Agents</th>\n",
       "      <th>RND</th>\n",
       "      <th>R</th>\n",
       "      <th>ACS</th>\n",
       "      <th>K:D</th>\n",
       "      <th>KAST</th>\n",
       "      <th>ADR</th>\n",
       "      <th>KPR</th>\n",
       "      <th>APR</th>\n",
       "      <th>...</th>\n",
       "      <th>FDPR</th>\n",
       "      <th>HS%</th>\n",
       "      <th>CL%</th>\n",
       "      <th>CL</th>\n",
       "      <th>KMAX</th>\n",
       "      <th>K</th>\n",
       "      <th>D</th>\n",
       "      <th>A</th>\n",
       "      <th>FK</th>\n",
       "      <th>FD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>invy\\r\\nTS</td>\n",
       "      <td>(+2)</td>\n",
       "      <td>271</td>\n",
       "      <td>1.26</td>\n",
       "      <td>237.2</td>\n",
       "      <td>1.27</td>\n",
       "      <td>73%</td>\n",
       "      <td>158.4</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>29%</td>\n",
       "      <td>10%</td>\n",
       "      <td>3/31</td>\n",
       "      <td>27</td>\n",
       "      <td>236</td>\n",
       "      <td>186</td>\n",
       "      <td>95</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jemkin\\r\\nRRQ</td>\n",
       "      <td>(+1)</td>\n",
       "      <td>573</td>\n",
       "      <td>1.22</td>\n",
       "      <td>253.9</td>\n",
       "      <td>1.30</td>\n",
       "      <td>73%</td>\n",
       "      <td>161.3</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15</td>\n",
       "      <td>38%</td>\n",
       "      <td>18%</td>\n",
       "      <td>9/50</td>\n",
       "      <td>32</td>\n",
       "      <td>524</td>\n",
       "      <td>404</td>\n",
       "      <td>87</td>\n",
       "      <td>137</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dos9\\r\\nBME</td>\n",
       "      <td>(+2)</td>\n",
       "      <td>344</td>\n",
       "      <td>1.19</td>\n",
       "      <td>212.2</td>\n",
       "      <td>1.18</td>\n",
       "      <td>75%</td>\n",
       "      <td>136.3</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>26%</td>\n",
       "      <td>29%</td>\n",
       "      <td>14/48</td>\n",
       "      <td>35</td>\n",
       "      <td>261</td>\n",
       "      <td>222</td>\n",
       "      <td>160</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jinggg\\r\\nPRX</td>\n",
       "      <td>(+2)</td>\n",
       "      <td>536</td>\n",
       "      <td>1.14</td>\n",
       "      <td>220.9</td>\n",
       "      <td>1.18</td>\n",
       "      <td>76%</td>\n",
       "      <td>145.4</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>26%</td>\n",
       "      <td>11%</td>\n",
       "      <td>7/62</td>\n",
       "      <td>31</td>\n",
       "      <td>422</td>\n",
       "      <td>359</td>\n",
       "      <td>171</td>\n",
       "      <td>53</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Foxy9\\r\\nGEN</td>\n",
       "      <td>(+2)</td>\n",
       "      <td>360</td>\n",
       "      <td>1.12</td>\n",
       "      <td>207.8</td>\n",
       "      <td>1.25</td>\n",
       "      <td>78%</td>\n",
       "      <td>137.3</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>34%</td>\n",
       "      <td>29%</td>\n",
       "      <td>9/31</td>\n",
       "      <td>23</td>\n",
       "      <td>277</td>\n",
       "      <td>222</td>\n",
       "      <td>65</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Jinboong\\r\\nDFM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>225</td>\n",
       "      <td>0.80</td>\n",
       "      <td>157.4</td>\n",
       "      <td>0.72</td>\n",
       "      <td>62%</td>\n",
       "      <td>111.1</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>33%</td>\n",
       "      <td>12%</td>\n",
       "      <td>4/33</td>\n",
       "      <td>16</td>\n",
       "      <td>121</td>\n",
       "      <td>169</td>\n",
       "      <td>48</td>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>SyouTa\\r\\nZETA</td>\n",
       "      <td>(+1)</td>\n",
       "      <td>255</td>\n",
       "      <td>0.79</td>\n",
       "      <td>162.8</td>\n",
       "      <td>0.77</td>\n",
       "      <td>61%</td>\n",
       "      <td>108.1</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>35%</td>\n",
       "      <td>9%</td>\n",
       "      <td>3/32</td>\n",
       "      <td>18</td>\n",
       "      <td>147</td>\n",
       "      <td>190</td>\n",
       "      <td>48</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Suggest\\r\\nGEN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112</td>\n",
       "      <td>0.66</td>\n",
       "      <td>152.2</td>\n",
       "      <td>0.77</td>\n",
       "      <td>65%</td>\n",
       "      <td>110.8</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>30%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/8</td>\n",
       "      <td>21</td>\n",
       "      <td>64</td>\n",
       "      <td>83</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Art\\r\\nDFM</td>\n",
       "      <td>(+1)</td>\n",
       "      <td>225</td>\n",
       "      <td>0.61</td>\n",
       "      <td>135.7</td>\n",
       "      <td>0.52</td>\n",
       "      <td>61%</td>\n",
       "      <td>92.8</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>21%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/18</td>\n",
       "      <td>16</td>\n",
       "      <td>94</td>\n",
       "      <td>182</td>\n",
       "      <td>69</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>patrickWHO\\r\\nGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121</td>\n",
       "      <td>0.55</td>\n",
       "      <td>152.7</td>\n",
       "      <td>0.56</td>\n",
       "      <td>60%</td>\n",
       "      <td>104.2</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>23%</td>\n",
       "      <td>13%</td>\n",
       "      <td>1/8</td>\n",
       "      <td>15</td>\n",
       "      <td>58</td>\n",
       "      <td>104</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Player Agents  RND     R    ACS   K:D KAST    ADR   KPR   APR  \\\n",
       "0         invy\\r\\nTS   (+2)  271  1.26  237.2  1.27  73%  158.4  0.87  0.35   \n",
       "1      Jemkin\\r\\nRRQ   (+1)  573  1.22  253.9  1.30  73%  161.3  0.91  0.15   \n",
       "2        dos9\\r\\nBME   (+2)  344  1.19  212.2  1.18  75%  136.3  0.76  0.47   \n",
       "3      Jinggg\\r\\nPRX   (+2)  536  1.14  220.9  1.18  76%  145.4  0.79  0.32   \n",
       "4       Foxy9\\r\\nGEN   (+2)  360  1.12  207.8  1.25  78%  137.3  0.77  0.18   \n",
       "..               ...    ...  ...   ...    ...   ...  ...    ...   ...   ...   \n",
       "60   Jinboong\\r\\nDFM    NaN  225  0.80  157.4  0.72  62%  111.1  0.54  0.21   \n",
       "61    SyouTa\\r\\nZETA   (+1)  255  0.79  162.8  0.77  61%  108.1  0.58  0.19   \n",
       "62    Suggest\\r\\nGEN    NaN  112  0.66  152.2  0.77  65%  110.8  0.57  0.12   \n",
       "63        Art\\r\\nDFM   (+1)  225  0.61  135.7  0.52  61%   92.8  0.42  0.31   \n",
       "64  patrickWHO\\r\\nGE    NaN  121  0.55  152.7  0.56  60%  104.2  0.48  0.17   \n",
       "\n",
       "    ...  FDPR  HS%  CL%     CL KMAX    K    D    A   FK  FD  \n",
       "0   ...  0.07  29%  10%   3/31   27  236  186   95   23  18  \n",
       "1   ...  0.15  38%  18%   9/50   32  524  404   87  137  84  \n",
       "2   ...  0.07  26%  29%  14/48   35  261  222  160   30  24  \n",
       "3   ...  0.08  26%  11%   7/62   31  422  359  171   53  43  \n",
       "4   ...  0.08  34%  29%   9/31   23  277  222   65   35  30  \n",
       "..  ...   ...  ...  ...    ...  ...  ...  ...  ...  ...  ..  \n",
       "60  ...  0.13  33%  12%   4/33   16  121  169   48   14  29  \n",
       "61  ...  0.07  35%   9%   3/32   18  147  190   48   26  18  \n",
       "62  ...  0.12  30%  NaN    0/8   21   64   83   13   10  13  \n",
       "63  ...  0.12  21%  NaN   0/18   16   94  182   69    9  28  \n",
       "64  ...  0.19  23%  13%    1/8   15   58  104   21   19  23  \n",
       "\n",
       "[65 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cek apakah file csv telah dibuat\n",
    "df = pd.read_csv('../data export/val_stats.csv')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
