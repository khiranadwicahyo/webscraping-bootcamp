{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f48804c",
   "metadata": {},
   "source": [
    "## **Basics Scraping with Scrapy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd72f25",
   "metadata": {},
   "source": [
    "### **Setup Environment and Packages**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9ee0c",
   "metadata": {},
   "source": [
    "Untuk memulai menggunakan Scrapy sangat disarankan untuk kita menggunakan virtual environment. Virtual environment merupakan sebuah ruang isolasi yang memungkinkan kita untuk menggunakannya secara virtual (terpisah dari penyimpanan global) untuk menyimpan segala kebutuhan packages yang akan digunakan pada suatu project. Hal ini penting karena akan memudahkan kita dalam memenuhi kebutuhan dari project satu dengan yang lainnya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec984e2f",
   "metadata": {},
   "source": [
    "Pertama-tama kita perlu setup dan install packages, dengan cara:\n",
    "\n",
    "**1. buka cmd/terminal/bash**\n",
    "**2. buat virtual environment**\n",
    " ```bash\n",
    "    python -m venv env # Windows # env merupakan nama virtual environement bisa berikan nama sesuai kebutuhan\n",
    "    python3 -m venv env # MacOS\n",
    "```\n",
    "**3. aktifkan virtual environment**\n",
    " ```bash\n",
    "    env\\Srcipts\\activate # Windows\n",
    "    source env\\bin\\activate # MacOS\n",
    "```\n",
    "**4. install requirements yang sudah disediakan atau install mandiri dengan menulis nama packages**\n",
    "```bash\n",
    "    pip install -r requirements.txt\n",
    "```\n",
    "**5. cek list packages yang terinstall**\n",
    "```bash\n",
    "    pip list\n",
    "```\n",
    "**6. deactivate jika sudah selesai menggunakan virtual environment**\n",
    "```bash\n",
    "    deactivate\n",
    "```\n",
    "\n",
    "**Tambahan**: Ketika anda menggunakan virtual enviroment, cek apakah sudah terdapat di jupyter kernel anda. Jika belum terdaftar di kernel, anda bisa menginstall ipykernel dan daftarkan env ke kernel dengan membuat name dan display name agar terlihat di vscode. Berikut perintah yang digunakan:\n",
    "\n",
    "```bash\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name=myenv --display-name \"Python (scrapy-env)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22963a7f",
   "metadata": {},
   "source": [
    "### **Setup Scrapy & Spiders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34e11ba",
   "metadata": {},
   "source": [
    "Pastikan env(virtual environment yang sudah dibuat) aktif dan berada di root directory project anda.\n",
    "\n",
    "**1. buat folder untuk project scrapy anda**\n",
    "```bash\n",
    "scrapy startproject scrapy_project\n",
    "```\n",
    "scrapy akan otomatis membuat folder sesuai nama yang dibuat dan telah berisi segala kebutuhan file python lainnya.\n",
    "\n",
    "```\n",
    "root_project/\n",
    "│\n",
    "├── scrapy_project/               <-- folder proyek scrapy\n",
    "│   ├── scrapy_project/\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── items.py\n",
    "│   │   ├── middlewares.py\n",
    "│   │   ├── pipelines.py\n",
    "│   │   ├── settings.py\n",
    "│   │   └── spiders/         <-- tempat kamu bikin file spider\n",
    "│   │       └── __init__.py\n",
    "│   └── scrapy.cfg           <-- konfigurasi utama scrapy\n",
    "│\n",
    "├── notebooks/               <-- folder kumpulan jupyter notebooks\n",
    "```\n",
    "\n",
    "**2. buat spiders untuk memulai melakukan project scraping**\n",
    "```bash\n",
    "scrapy genspiders exapmple example.com\n",
    "```\n",
    "perintah diatas akan membuat nama spider **example** dan membuat template scrapy sesuai url yang menjadi target scraping yaitu **example.com**.   \n",
    "\n",
    "```\n",
    "scrapy_project/               <-- folder proyek scrapy\n",
    "    ├── scrapy_project/\n",
    "    │   ├── __pycahche__/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── items.py\n",
    "    │   ├── middlewares.py\n",
    "    │   ├── pipelines.py\n",
    "    │   ├── settings.py\n",
    "    │   └── spiders/         <-- tempat kamu bikin file spider\n",
    "    │       ├──__pycahche__/\n",
    "    │       ├──__init__.py\n",
    "    │       └── example.py\n",
    "    └── scrapy.cfg\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc84c1",
   "metadata": {},
   "source": [
    "**Template isi file exapmle.py**\n",
    "\n",
    "```Python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "    name = \"example\"\n",
    "    allowed_domains = [\"www.example.info\"]\n",
    "    start_urls = [\"https://www.example.info/world-population/population-by-country\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        yield\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae125aa3",
   "metadata": {},
   "source": [
    "## **Scrapy Bascics for Scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18434c2b",
   "metadata": {},
   "source": [
    "![worldometers population](../media/worldometers_scrapy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c336dae",
   "metadata": {},
   "source": [
    "Untuk memulai buka terminal/cmd anda dan pastikan env sudah aktif dan directory berada pada folder project scrapy anda.\n",
    "\n",
    "Jika semua sudah siap, anda bisa memulai dengan mengetahui beberapa dasar awal yang sering digunakan.\n",
    "\n",
    "**1. `scrapy shell`**\n",
    "Perintah ini akan membuat kita memasuki tools interaktif dari Scrapy untuk melakukan penujian dan debug sebelum melakukan scraping. Kita kan memulai dengan melakukan request terhadap target web yang ingin discraping.\n",
    "\n",
    "Dalam kasus ini saya menggunakan website worldometers.info. Perintah yang dilakukan sebagai berikut:\n",
    "```bash\n",
    "scrapy shell\n",
    "In [1]: r = scrapy.Request('https://www.worldometers.info/world-population/population-by-country/')\n",
    "In [2]: fetch(r)\n",
    "```\n",
    "Perintah Request adalah untuk mendapatkan request dari website yang kita tuju dan fetch untuk mendapatkan response untuk hasilnya.\n",
    "\n",
    "**2. lakukan perintah untuk mendapatkan hasil dari website**\n",
    "```bash\n",
    "In [3]: response.body\n",
    "\n",
    "In [4]: response.xpath('//h1')\n",
    "Out[4]: [<Selector query='//h1' data='<h1 id=\"countries-in-the-world-by-pop...'>]\n",
    "\n",
    "In [5]: response.xpath('//h1/text()')\n",
    "Out[5]: [<Selector query='//h1/text()' data='Countries in the world by population ...'>]\n",
    "\n",
    "In [6]: response.xpath('//h1/text()').get()\n",
    "Out[6]: 'Countries in the world by population (2025)'\n",
    "\n",
    "In [7]: response.xpath('//td/a/text()').get()\n",
    "Out[7]: 'India'\n",
    "\n",
    "In [8]: response.xpath('//td/a/text()').getall()\n",
    "Out[8]:\n",
    "['India',\n",
    " 'China',\n",
    " 'United States',\n",
    " 'Indonesia',\n",
    " 'Pakistan',\n",
    " 'Nigeria',\n",
    " ...]\n",
    "```\n",
    "`response.body` = untuk mengambil hasil dari seluruh body struktur HTML website\n",
    "\n",
    "`response.xpath` = untuk mengambil hasil dari xpath yang diuju\n",
    "\n",
    "`/text()` = cara untuk mengambil teks yang terdapat pada suatu tag/path yang ada di stuktur HTML\n",
    "\n",
    "`.get/.getall` = untuk mengamil hail/untuk mengamil semua hasil yang memiliki xpath yang sama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd366395",
   "metadata": {},
   "source": [
    "Jika sudah dilakukan testing di Scrapy shell kita bisa mulai memasukkan kode kedalam project kita, dan running.\n",
    "\n",
    "```Python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class WorldometersSpider(scrapy.Spider):\n",
    "    name = \"worldometers\"\n",
    "    allowed_domains = [\"www.worldometers.info\"]\n",
    "    start_urls = [\"https://www.worldometers.info/world-population/population-by-country\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.xpath('//h1/text()').get()\n",
    "        countries = response.xpath('//td/a').getall()\n",
    "\n",
    "            \n",
    "        yield {\n",
    "                'title' : title,\n",
    "                'country': countries,\n",
    "        }\n",
    "```\n",
    "\n",
    "Untuk running anda bisa gunakan perintah pada scrapy:\n",
    "```bash\n",
    "scrapy crawl worldometers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2926f30",
   "metadata": {},
   "source": [
    "### **Scraping Data from Multiple Links** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882554d9",
   "metadata": {},
   "source": [
    "Sebelum mengerti mekanisme untuk mengambil link menggunakan Scrapy, kita perlu tahu apa itu **absolute** dan **relative** link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5805637",
   "metadata": {},
   "source": [
    "### **Absolute & Relative Link**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2421f8",
   "metadata": {},
   "source": [
    "Relative link merupakan tautan link yang tidak menyertakan alamat lengkap URL teramasuk nama domain dan protokolnya. Realative link hanya menunjukkan jalur menuju sumber halaman yang dituju. Berbeda dengan absolute link yang merupakan link lengkap dengan domain dan protokolnya. Contoh link untuk mamahami absolute dan relative link.\n",
    "\n",
    "**Absolute link** : \"https//:exapmle.com\"\n",
    "\n",
    "**Relative link** : \"/example-info/example-1.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781bcf68",
   "metadata": {},
   "source": [
    "### **Get Multiple Link**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e2d0c6",
   "metadata": {},
   "source": [
    "![worldometers-table](../media/worldometers_table_scrapy.png)\n",
    "\n",
    "Jika kita lihat dari href dapat kita lihat relative link merujuk kepada halaman khusus pada negara India.\n",
    "\n",
    "![worldometers-relative_link](../media/worldometers_scrapy_relative_link.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a073907",
   "metadata": {},
   "source": [
    "Dalam Scrapy untuk bisa melakukan request pada relative link, ada beberapa cara.\n",
    "\n",
    "Pertama-tama kita perlu mengambil attribut href dengan cara\n",
    "\n",
    "Cara manual, ktia bisa melakukan inisiasi absolute link + relative link, lalu melakukan request.\n",
    "\n",
    "Tetapi di Scrapy kita bisa mudah melakukannya tanpa harus melakukannya secara manual, kita hanya perlu menggunakan fungsi `follow(url)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6600cb5d",
   "metadata": {},
   "source": [
    "Contoh kodenya akan seperti ini:\n",
    "```Python\n",
    "class WorldometersSpider(scrapy.Spider):\n",
    "    name = \"worldometers\"\n",
    "    allowed_domains = [\"www.worldometers.info\"]\n",
    "    start_urls = [\"https://www.worldometers.info/world-population/population-by-country\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # title = response.xpath('//h1/text()').get()\n",
    "        countries = response.xpath('//td/a')\n",
    "\n",
    "        for country in countries:\n",
    "            country_name = country.xpath('.//text()').get()\n",
    "            link = country.xpath('.//@href').get()\n",
    "            \n",
    "\n",
    "\n",
    "            # untuk menngambil link lengkap ada dua cara\n",
    "\n",
    "            # 1. Menggunakan response.urljoin\n",
    "            # absoulute_url = f'https://www.worldometers.info{link}'\n",
    "            # yield response.urljoin(url=abosolute_url)\n",
    "\n",
    "            # 2. Menggunakan response.follow\n",
    "            # yield response.follow(url=link) # cukup gunakan link relatif\n",
    "\n",
    "            # untuk mengembalikan data dalam bentuk dictionary\n",
    "            yield {\n",
    "                \"countries\": country_name,\n",
    "                \"links\": link,\n",
    "            }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41d6cb6",
   "metadata": {},
   "source": [
    "### **Request to Each Link and Crawl The Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd2f5ac",
   "metadata": {},
   "source": [
    "```Python\n",
    "class WorldometersSpider(scrapy.Spider):\n",
    "    name = \"worldometers\"\n",
    "    allowed_domains = [\"www.worldometers.info\"]\n",
    "    start_urls = [\"https://www.worldometers.info/world-population/population-by-country\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # title = response.xpath('//h1/text()').get()\n",
    "        countries = response.xpath('//td/a')\n",
    "\n",
    "        for country in countries:\n",
    "            country_name = country.xpath('.//text()').get()\n",
    "            link = country.xpath('.//@href').get()\n",
    "\n",
    "            yield response.follow(url=link, callback=self.parse_country, meta={\"country\": country_name})\n",
    "\n",
    "    def parse_country(self, response):\n",
    "        # untuk mengambil data dari halaman negara\n",
    "        rows = response.xpath('(//table[contains(@class, \"table\")])[1]//tbody/tr')\n",
    "\n",
    "        country = response.request.meta['country']\n",
    "        for row in rows:\n",
    "            year = row.xpath('.//td[1]/text()').get()\n",
    "            population = row.xpath('.//td[2]/text()').get()\n",
    "\n",
    "            yield {\n",
    "                \"country\": country,\n",
    "                \"year\": year,\n",
    "                \"population\": population,\n",
    "            }\n",
    "```\n",
    "\n",
    "Kode di atas melakuan crawling pada xpath awal sepeti ini:\n",
    "```xpath\n",
    "(//table[@class=\"datatable w-full border border-zinc-200 datatable-table\"])[1]\n",
    "```\n",
    "\n",
    "Untuk mengambil data dari setiap baris tabel yang terdapat pada tag `<tr>` dan `<td>` kita perlu masukkan xpath sesuai dengan data yang ingin diambil. Namun, kita perlu membuat fungsi tersendiri untuk mengambil data dari relative link yang sudah didapat. \n",
    "\n",
    "Ketika sudah membuat fungsi untuk crawling data pada relative link, \n",
    "```Python\n",
    "yield response.follow(url=link, callback=self.parse_country, meta={\"country\": country_name})\n",
    "``` \n",
    "baris kode ini akan melakukan request dan mendapatkan respon dari link yang didapatkan dan callback pada fungsi yang sudah dibuat untuk mengambil data, meta berfungsi sebagai data tambahan yang sudah kita ambil sebelumnya untuk menghubungkan data yang telah diambil sebelumnya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692d984",
   "metadata": {},
   "source": [
    "Untuk melakukan running kita bisa memulai dengan crawling\n",
    "```bash\n",
    "scrapy crawls worldometers\n",
    "```\n",
    "Perhatikan jika kode response (200) maka data berhasil di crawling. Sekarang untuk bisa melakukan exporting data, kita hanya perlu melakukan perintah pada env kita dengan:\n",
    "```bash\n",
    "scrapy crawls -o data_export_exampple.csv\n",
    "```\n",
    "atau \n",
    "```bash\n",
    "scrapy crawls -o \"path_folder/data_export_example.json\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scrapy-env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
