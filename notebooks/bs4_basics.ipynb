{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "296493d7",
   "metadata": {},
   "source": [
    "## **Basics Scraping With Beautifulsoup4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df00f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd32d4d5",
   "metadata": {},
   "source": [
    "### **What we need to know about Beautifulsoup4 (Bs4)?**\n",
    "\n",
    "Bs4 merupakan librarry yang digunakan untuk melakukan web scraping pada suatu website. Dalam penggunaan Bs4 kita bergantung pada *Request* dari suatu website untuk mendapatkan data yang kita mau. Meskipun terlihat simpel Bs4 merupakan web scraper yang juga membutuhkan *parser* seperti **html parser** atau **xml parser** untuk mengekstrak data dari suatu website. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9981cc09",
   "metadata": {},
   "source": [
    "### **Scraping Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b417b",
   "metadata": {},
   "source": [
    "Biasanya dalam scraping menggunakan Bs4 kita membutuhkan:\n",
    "\n",
    "    1. Inisiasi URL\n",
    "    2. Buat Request\n",
    "    3. Parsing Data\n",
    "    4. Ekstraksi Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73df780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat insisiaisi website\n",
    "website = 'https://subslikescript.com/movie/The_Love_Club-19896920'\n",
    "# melakukan request ke website\n",
    "result = requests.get(website)\n",
    "# menggambil konten dari hasil request\n",
    "content = result.text\n",
    "\n",
    "# membuat soup untuk parsing lxml/html\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "# memuncuklan hasil parsing\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e6317",
   "metadata": {},
   "source": [
    "**Screenshot website yang ingin dilakukan scraping**\n",
    "\n",
    "![media - website subslikescript](../media/basics_scraping_subslikescript.png)\n",
    "\n",
    "Untuk melihat elemen — kita bisa melakukan inspect langsung pada website dan pilih elemen yang ingin diambil datanya. Pada contoh kali ini saya ingin mengambil data judul dari film \"THE LOVE CLUB (2023) - FULL TRANSCRIPT\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082aa3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Love Club (2023) - full transcript\n"
     ]
    }
   ],
   "source": [
    "# karena judul film berada di dalam tag <h1> dan <article>\n",
    "# kita perlu mencari tag sebelumnya yaitu <article>\n",
    "box = soup.find('article', class_='main-article')\n",
    "# mengambil judul film dari tag <h1>\n",
    "title = box.find('h1').text\n",
    "# print hasil judul film\n",
    "print(f'Title: {title}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1662bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selanjutnya kita akan mengambil transkrip film dan membuat file txtnya\n",
    "\n",
    "# mengambil transkrip film dari tag <div> dengan class 'full-script'\n",
    "transcript = box.find('div', class_='full-script').get_text(strip=True, separator=' ')\n",
    "\n",
    "# membuat file\n",
    "# ketika membuat file, gunakan mode 'w' untuk menulis\n",
    "# dan encoding 'utf-8' untuk mendukung karakter non-ASCII\n",
    "# pastikan direktori '../data export/' sudah ada\n",
    "# jika belum ada, buat direktori tersebut terlebih dahulu\n",
    "# dalah contoh ini, kita akan menyimpan file dengan nama sesuai judul film\n",
    "with open(f'../data export/{title}.txt', 'w', encoding= 'utf-8') as file:\n",
    "    file.write(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc80c5d",
   "metadata": {},
   "source": [
    "### **Scraping Multiple Element**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e26b172",
   "metadata": {},
   "source": [
    "![subslikescript - multiple element href true](../media/href_true.png)\n",
    "\n",
    "\n",
    "![subslikescript - href zoom](../media/href_zoom.png)\n",
    "\n",
    "Dalam contoh kali ini, untuk mengambil beberapa element yang sama kita perlu memahami struktur html dari suatu website. Jika diperhatikan **href** menunjukkan bahwa setiap movies akan menampilkan tampilan transcript dihalaman yang terpisah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2339c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/movie/Gereza-22857380', '/movie/Dragon_Hunter-22008250', '/movie/House_of_Inequity-5598934', '/movie/Jackpot_Island_Kumanthong_Returns-25260658', '/movie/Match-13490000', '/movie/Snow_White_and_the_Fairytale_Fun_Force-22700808', '/movie/The_Love_Club-19896920', '/movie/Haunted_Hotties-24018636', '/movie/One_Year_Off-14130940', '/movie/Sweeter_Than_Chocolate-25391002', '/movie/Disquiet-25869142', '/movie/Among_the_Beasts-26343318', '/movie/Taming_Speed-25969676', '/movie/Randy_Feltface_The_Last_Temptation_of_Randy-17527566', '/movie/Alemanji-23666980', '/movie/Darkheart_Manor-12569866', '/movie/Innocent_Vengeance-26505553', '/movie/Ho_Ja_Mukt-25150506', '/movie/Boy_from_Nowhere-26713915', '/movie/Rent-a-Groom-15352290', '/movie/Unlocked-26160190', '/movie/j-hope_IN_THE_BOX-26425683', '/movie/Suki-26734145', '/movie/Re-Resonator_Looking_Back_at_from_Beyond-26625455', '/movie/Americas_National_Parks_at_100-26740893', '/movie/Under_His_Influence-26083016', '/movie/Why_Cant_My_Life_Be_a_Rom_Com-20912894', '/movie/Nasty-22500124', '/movie/Kathleen_Madigan_Hunting_Bigfoot-26348233', '/movie/Cassius_X_Becoming_Ali-17001434']\n"
     ]
    }
   ],
   "source": [
    "# membuat insisiaisi website\n",
    "website = 'https://subslikescript.com/movies'\n",
    "# melakukan request ke website\n",
    "result = requests.get(website)\n",
    "# menggambil konten dari hasil request\n",
    "content = result.text\n",
    "\n",
    "# membuat soup untuk parsing lxml/html\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "box = soup.find('article', class_='main-article')\n",
    "\n",
    "links = []\n",
    "\n",
    "for link in box.find_all('a', href=True):\n",
    "    # mengambil link dari atribut href\n",
    "    links.append(link['href'])\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65bf24c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: \"../data export/Why Can't My Life Be a Rom Com? (2023) - full transcript.txt\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn [17], line 18\u001b[0m\n",
      "\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# mengambil transkrip film dari tag <div> dengan class 'full-script'\u001b[39;00m\n",
      "\u001b[0;32m     16\u001b[0m     transcript \u001b[38;5;241m=\u001b[39m box\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull-script\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data export/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtitle\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[0;32m     19\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(transcript)\n",
      "\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# menampilkan pesan selesai\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: \"../data export/Why Can't My Life Be a Rom Com? (2023) - full transcript.txt\""
     ]
    }
   ],
   "source": [
    "root = 'https://subslikescript.com'\n",
    "# mengambil setiap link dan mengambil transkrip filmnya menggunakan loop\n",
    "for link in links:\n",
    "    movies = f'{root}{link}'\n",
    "    # melakukan request ke website\n",
    "    result = requests.get(movies)\n",
    "    # menggambil konten dari hasil request\n",
    "    content = result.text\n",
    "    # membuat soup untuk parsing lxml/html\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "    box = soup.find('article', class_='main-article')\n",
    "    # mengambil judul film dari tag <h1>\n",
    "    title = box.find('h1').text\n",
    "    # mengambil transkrip film dari tag <div> dengan class 'full-script'\n",
    "    transcript = box.find('div', class_='full-script').get_text(strip=True, separator=' ')\n",
    "\n",
    "    with open(f'../data export/{title}.txt', 'w', encoding= 'utf-8') as file:\n",
    "        file.write(transcript)\n",
    "        \n",
    "# menampilkan pesan selesai\n",
    "print('Semua transkrip film telah diambil dan disimpan!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d850a50",
   "metadata": {},
   "source": [
    "Jika kita lihat diatas terdapat error yang disebabkan karena adanya karakter ilegal untuk membuat nama file. Kita bisa menggunakan try-except untuk solusi seperti ini atau gunakaan fungsi regex untuk membersihkan karakter yang tidak aman, tetapi fokus utama dalam mengambil multiple element sudah berhasil diterapkan. jika kita cek di directory file sudah berhasil dibuat dan diambil datanya.\n",
    "\n",
    "![multiple element scraping](../media/multiple_element_scraping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354623d",
   "metadata": {},
   "source": [
    "### **Pagination Using Bs4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9563815f",
   "metadata": {},
   "source": [
    "![pagination element](../media/pagination.png)\n",
    "\n",
    "Untuk melakukan pagination kita perlu mangambil elemen yang memuat akses ke halama berikutnya. Dalam website \"subslikescript.com\" dapan dilihat elemen berapa pada box ul dan link pada li. Jika diperhatikan juga situs akan berubah menjadi \"subslikescript.com/**movies?page=2**\" artinya setiap perubahan ke halaman berikutnya situs akan berubah menjadi seperti ini. Jika sudah diketahui karakteristik dari web ketika berpindah ke halaman berikutnya kita bisa mengakali dengan mengambil banyak halaman dan memasukkannya dengan angka pada **page={}**.\n",
    "\n",
    "Sayangnya website ini telah membatasi aktivitas seperti ini, karena sistem website akan mengenali pergerakan tidak wajar dari user. Walaupun mungkin bisa diatasi menggunakan fungsi time untuk melakukan jeda pada proses scraping, akan tetapi tidak ada jaminan data dapat terambil semua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a740f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1bad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dabb2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transkrip untuk Gereza (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Dragon Hunter (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk House of Inequity (2021) - full transcript telah disimpan.\n",
      "Transkrip untuk Jackpot Island Kumanthong Returns (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Match (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Snow White and the Fairytale Fun Force (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk The Love Club (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Haunted Hotties (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk One Year Off (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Sweeter Than Chocolate (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Disquiet (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Among the Beasts (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Taming Speed (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Randy Feltface The Last Temptation of Randy (2020) - full transcript telah disimpan.\n",
      "Transkrip untuk Alemanji (2021) - full transcript telah disimpan.\n",
      "Transkrip untuk Darkheart Manor (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Innocent Vengeance (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Ho Ja Mukt (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Boy from Nowhere (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Rent-a-Groom (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Unlocked (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk j-hope IN THE BOX (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Suki (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Re-Resonator Looking Back at from Beyond (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk America's National Parks at 100 (2016) - full transcript telah disimpan.\n",
      "Transkrip untuk Under His Influence (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Why Can't My Life Be a Rom Com (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Nasty (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Kathleen Madigan Hunting Bigfoot (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Cassius X Becoming Ali (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Verden er uskarp (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Bullet to Beijing (1995) - full transcript telah disimpan.\n",
      "Transkrip untuk Whindersson Nunes Isso Não é um Culto (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Fursat (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk The Weeknd Live at SoFi Stadium (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk The Adventures of Jurassic Pet The Lost Secret (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Casando a mi Ex (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Tonight You're Sleeping with Me (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Batman The Doom That Came to Gotham (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk South Park Post COVID - The Return of COVID (2021) - full transcript telah disimpan.\n",
      "Transkrip untuk Fall (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Black Girl Missing (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Royal Rendezvous (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Love Sick Open All Day, Every Night (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Marc Maron From Bleak to Dark (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Pink Moon (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Lady Behave! (1937) - full transcript telah disimpan.\n",
      "Transkrip untuk The Legend & Butterfly (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Kalevala - uusi aika (2013) - full transcript telah disimpan.\n",
      "Transkrip untuk Face Off The Walking Guests () - full transcript telah disimpan.\n",
      "Transkrip untuk Revenge of the First Wives (1997) - full transcript telah disimpan.\n",
      "Transkrip untuk The London Nobody Knows (1968) - full transcript telah disimpan.\n",
      "Transkrip untuk Building Ireland (2017) - full transcript telah disimpan.\n",
      "Transkrip untuk The Girl on a Motorcycle (1968) - full transcript telah disimpan.\n",
      "Transkrip untuk Taz Quest for Burger (2023) - full transcript telah disimpan.\n",
      "Transkrip untuk Trust Us (2022) - full transcript telah disimpan.\n",
      "Transkrip untuk Swooner Crooner (1944) - full transcript telah disimpan.\n",
      "Transkrip untuk Plane Daffy (1944) - full transcript telah disimpan.\n",
      "Transkrip untuk A Gruesome Twosome (1945) - full transcript telah disimpan.\n",
      "Transkrip untuk I Am T-Rex (2022) - full transcript telah disimpan.\n"
     ]
    }
   ],
   "source": [
    "root = 'https://subslikescript.com'\n",
    "base_url = 'https://subslikescript.com/movies'\n",
    "result = requests.get(base_url)\n",
    "# menggambil konten dari hasil request\n",
    "content = result.text\n",
    "\n",
    "# membuat soup untuk parsing lxml/html\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "# pagination adalah elemen yang berisi link ke halaman-halaman berikutnya\n",
    "pagination = soup.find('ul', class_='pagination')\n",
    "pages = pagination.find_all('li', class_='page-item')\n",
    "last_pages = pages[-2].text  # mengambil jumlah halaman terakhir\n",
    "\n",
    "\n",
    "# for page in range(1, int(last_pages) + 1)[:2]: # [:2] mengambil 2 halaman pertama\n",
    "for page in range(1, 3): \n",
    "    try:\n",
    "        get_ref = requests.get(f'{base_url}?page={page}')\n",
    "        pg_content = get_ref.text\n",
    "        soup = BeautifulSoup(pg_content, 'lxml')\n",
    "\n",
    "        box = soup.find('article', class_='main-article')\n",
    "        # mengambil link dari setiap halaman\n",
    "        # for link in box.find_all('a', href=True):\n",
    "        # # mengambil link dari atribut href\n",
    "        #     links.append(link['href'])\n",
    "\n",
    "        # reset link untuk tiap halaman\n",
    "        links = [link['href'] for link in box.find_all('a', href=True)]\n",
    "    \n",
    "        # mengambil setiap link dan mengambil transkrip filmnya menggunakan loop\n",
    "        for link in links:\n",
    "            try:\n",
    "                # melakukan request ke website menggunakan link yang telah diambil\n",
    "                result = requests.get(f'{root}{link}')\n",
    "                time.sleep(2)  # menunggu 2 detik untuk menghindari terlalu banyak request\n",
    "                # menggambil konten dari hasil request\n",
    "                content = result.text\n",
    "                # membuat soup untuk parsing lxml/html\n",
    "                soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "                box = soup.find('article', class_='main-article')\n",
    "                # mengambil judul film dari tag <h1>\n",
    "                title = box.find('h1').text\n",
    "                # membersihkan judul film dari karakter yang tidak diinginkan\n",
    "                safe_title = clean_text(title)\n",
    "                # mengambil transkrip film dari tag <div> dengan class 'full-script'\n",
    "                transcript = box.find('div', class_='full-script').get_text(strip=True, separator=' ')\n",
    "\n",
    "                with open(f'../data export/{safe_title}.txt', 'w', encoding= 'utf-8') as file:\n",
    "                    file.write(transcript)\n",
    "                print(f'Transkrip untuk {safe_title} telah disimpan.')\n",
    "            except Exception as e:\n",
    "                print(f'Error pada link {link}: {e}')\n",
    "                continue\n",
    "    except:\n",
    "        print(f'Error pada halaman {page}, melanjutkan ke halaman berikutnya.')\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
